---
title: Iterators and Generators
layout: post
---

<span class="firstcharacter">T</span>rhe main benefit of using iterators is the memory efficiency. _Iterators do not compute the values of the sequence until they are needed_. This is especially useful when working with large sequences or datasets.

Let's say you have 10GB of data stored in a file. If you read the entire file into memory, you would need 10GB of memory to store the data. However, if you use an iterator to read the file line by line, you only need enough memory to store one line at a time.

Here is the code to read a file line by line using an iterator:

```python
from typing import TextIO, Iterator

def read_large_file(file: TextIO) -> Iterator[str]:
    with open(file, 'r') as f:
        for line in f:
            yield line.strip()

def process_lines(lines: Iterator[str]) -> None:
    for line in lines:
        # Process the line
        yield line.upper()

file = 'large_file.txt'
lines = read_large_file(file)
processed_lines = process_lines(lines)

for line in processed_lines:
    print(line)
```

With this kind of code, you can process large files without running out of memory. The `read_large_file` function reads the file line by line and yields each line as it is read. The `process_lines` function processes each line and yields the processed line. The `for` loop at the end iterates over the processed lines and prints them. It is important to know that one has to use context managers to open files in the `read_large_file` function to ensure that the file is closed properly after reading.


Whether you are working as a _data scientist_ or _natural language processing engineer_, you will often encounter situations where you need to process large datasets. Say we have a csv file with 1 million rows and we want to process each row. 

Here is the common pattern for processing large `csv` files:

```python
import csv
import pandas as pd
from typing import TextIO, Iterator

def read_large_csv(file: TextIO) -> Iterator[dict]:
    with open(file, 'r') as f:
        for lines in f:
            yield lines.strip().split(',')

def check_first_few_rows(file: TextIO, n: int) -> None:
    # this function reads the first n rows of the csv file
    # we often use this function to check the structure of the csv file
    # and to check headers
    lines = read_large_csv(file)
    for i, line in enumerate(lines):
        if i < n:
            print(line)

def large_csv_reader(file: TextIO) -> None:
    # we can also use csv.reader to read csv files
    with open(file, 'r') as f:
        reader = csv.reader(f)
        for row in reader:
            yield row

def read_csv_chunk(file: TextIO, chunksize: int) -> None:
    # we can also use pandas to read csv files in chunks
    # this is useful when we want to process the csv file in chunks
    # instead of reading the entire file into memory
    reader = pd.read_csv(file, chunksize=chunksize)
    for chunk in reader:
        yield chunk

```

## Streaming Data

Most of data scientists and machine learning engineers do not work with streaming data. Our daily work involves working with static datasets, such as fetching data from databases, reading csv files, or reading data from APIs. However, there are situations where we need to work with streaming data. For example, when we are working on real-time analytics, we need to process data as it arrives. 

Even if you do not work with streaming data, have a basic understanding of how to work with streaming data is important. For instance, one of the projects I worked on involved reading a large dataset from cloud storage. It is a parquet file around 20 Gigabytes. I used `Spark` to read the file and process the data. 

Since `Spark` has built-in support for reading large datasets, we do not need to worry about memory issues. However, after reading the data, I needed to do downstream processing with either `pandas` or `numpy`. This is when I realized that I need to process the data in chunks or batches (this kind of situation is very common in data engineering).

Now, let's use a simple example to illustrate how to work with streaming data.


```text
Anscombe's quartet
I	II	III	IV
x	y	x	y	x	y	x	y
10.0	8.04	10.0	9.14	10.0	7.46	8.0	6.58
8.0	6.95	8.0	8.14	8.0	6.77	8.0	5.76
13.0	7.58	13.0	8.74	13.0	12.74	8.0	7.71
9.0	8.81	9.0	8.77	9.0	7.11	8.0	8.84
11.0	8.33	11.0	9.26	11.0	7.81	8.0	8.47
14.0	9.96	14.0	8.10	14.0	8.84	8.0	7.04
6.0	7.24	6.0	6.13	6.0	6.08	8.0	5.25
4.0	4.26	4.0	3.10	4.0	5.39	19.0	12.50
12.0	10.84	12.0	9.13	12.0	8.15	8.0	5.56
7.0	4.82	7.0	7.26	7.0	6.42	8.0	7.91
5.0	5.68	5.0	4.74	5.0	5.73	8.0	6.89
```