---
title: Iterators and Generators
layout: post
---

The main benefit of using iterators is the memory efficiency. Iterators do not compute the values of the sequence until they are needed. This is especially useful when working with large sequences or datasets.

Let's say you have 10GB of data stored in a file. If you read the entire file into memory, you would need 10GB of memory to store the data. However, if you use an iterator to read the file line by line, you only need enough memory to store one line at a time.

Here is the code to read a file line by line using an iterator:

```python
from typing import TextIO, Iterator

def read_large_file(file: TextIO) -> Iterator[str]:
    with open(file, 'r') as f:
        for line in f:
            yield line.strip()

def process_lines(lines: Iterator[str]) -> None:
    for line in lines:
        # Process the line
        yield line.upper()

file = 'large_file.txt'
lines = read_large_file(file)
processed_lines = process_lines(lines)

for line in processed_lines:
    print(line)
```

With this kind of code, you can process large files without running out of memory. The `read_large_file` function reads the file line by line and yields each line as it is read. The `process_lines` function processes each line and yields the processed line. The `for` loop at the end iterates over the processed lines and prints them. It is important to know that one has to use context managers to open files in the `read_large_file` function to ensure that the file is closed properly after reading.


Whether you are working as a _data scientist_ or _natural language processing engineer_, you will often encounter situations where you need to process large datasets. Say we have a csv file with 1 million rows and we want to process each row. 

Here is the common pattern for processing large `csv` files:

```python
import csv
import pandas as pd
from typing import TextIO, Iterator


def read_large_csv(file: TextIO) -> Iterator[dict]:
    with open(file, 'r') as f:
        for lines in f:
            yield lines.strip().split(',')


def check_first_few_rows(file: TextIO, n: int) -> None:
    # this function reads the first n rows of the csv file
    # we often use this function to check the structure of the csv file
    # and to check headers
    lines = read_large_csv(file)
    for i, line in enumerate(lines):
        if i < n:
            print(line)


def lareg_csv_reader(file: TextIO) -> None:
    # we can also use csv.reader to read csv files
    with open(file, 'r') as f:
        reader = csv.reader(f)
        for row in reader:
            yield row


def read_csv_chunk(file: TextIO, chunksize: int) -> None:
    # we can also use pandas to read csv files in chunks
    # this is useful when we want to process the csv file in chunks
    # instead of reading the entire file into memory
    reader = pd.read_csv(file, chunksize=chunksize)
    for chunk in reader:
        yield chunk

```